Supabase (free tier) scheduled scrape + storage steps

1) Create a Supabase project
   - Go to https://supabase.com and create a new project.
   - Note the Project URL and project ID.
   - Example Project URL (placeholder): https://tokzbiepijjdvbdtacjz.supabase.co

2) Create a Storage bucket
   - Supabase Dashboard → Storage → Create bucket.
   - Name it: jobs
   - Set it to Public if you want unauthenticated reads.

3) Identify which keys to use
   - GitHub Actions upload (server-side): use the SECRET key.
     - Example SECRET key (placeholder): sb_secret_zx4B5VXb0I6P9pMqt9ggPA_oKQNQTWK
   - Frontend read (browser): use the public URL, no key required if the bucket is public.
     - Example PUBLISHABLE key (placeholder): sb_publishable_02E0zOnHgDgUcfzwjn224g_CJiBXjAR
   - Do NOT commit real keys to the repo.

4) Add GitHub Actions secrets
   - Repo → Settings → Secrets and variables → Actions.
   - Add SUPABASE_URL (Project URL).
     - Example: https://tokzbiepijjdvbdtacjz.supabase.co
   - Add SUPABASE_SERVICE_ROLE_KEY (your Secret key).
     - Example: sb_secret_zx4B5VXb0I6P9pMqt9ggPA_oKQNQTWK

5) Add a scrape script (no Flask server)
   - Create scripts/scrape_jobs.py
   - It should import the scraping functions from server.py and write jobs.json
     to disk, e.g. {"updated_at": "...", "count": N, "jobs": [...]}.

6) Add a GitHub Actions workflow
   - Create .github/workflows/scrape.yml
   - Schedule it with cron (e.g., every 6 hours):
     on:
       schedule:
         - cron: "0 */6 * * *"
       workflow_dispatch:
   - Steps:
     - Checkout repo
     - Setup Python
     - Install deps (pip install -r requirements.txt or explicit packages)
     - Run: python scripts/scrape_jobs.py
     - Upload jobs.json to Supabase Storage

7) Upload jobs.json to Supabase Storage (example curl)
   - PUT https://tokzbiepijjdvbdtacjz.supabase.co/storage/v1/object/jobs/jobs.json
   - Headers:
     - Authorization: Bearer $SUPABASE_SERVICE_ROLE_KEY
     - apikey: $SUPABASE_SERVICE_ROLE_KEY
     - Content-Type: application/json
   - Body: jobs.json

8) Use the public URL in the frontend
   - Public URL format:
     https://tokzbiepijjdvbdtacjz.supabase.co/storage/v1/object/public/jobs/jobs.json
   - Update website/script.js to fetch from this URL.

9) Stop committing cache files
   - Add jobs_cache.json and jobs_snapshot.json to .gitignore.
   - Delete them from the repo once workflow is verified.

10) Validate
   - Manually trigger the workflow.
   - Confirm jobs.json updates in Supabase Storage.
   - Load the site and confirm data is fresh.
